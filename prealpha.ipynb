{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f4d5cbe",
   "metadata": {},
   "source": [
    "Link to current Kaggle dataset: https://www.kaggle.com/datasets/enesbayturk/vehicle-and-pedestrian-detection-dataset\n",
    "\n",
    "8 classes, images of big truck, small truck, bus, car, motorcycle, van, and pedestrian. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f417456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import datetime\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "from torch.nn import Flatten, Linear, Conv2d, MaxPool2d, Dropout, Sequential\n",
    "from torch.nn import ReLU, Sigmoid, Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74515102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f07391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5), # Data augmentation - horizontal flip, rotation, color properties\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.Resize((128, 128)),  # Resize all images to the same size\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "regular_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load images, resize all to 128x128 pixels, save as tensors\n",
    "dataset_unaug = datasets.ImageFolder(root='project_data', transform = regular_transform)\n",
    "dataset_aug = datasets.ImageFolder(root='project_data', transform=aug_transform)\n",
    "\n",
    "# Split Data into training and valtest\n",
    "# Dont split augmented dataset, bc all of that will be for training\n",
    "n_train = len(dataset_unaug) * 0.5\n",
    "n_train = int(n_train)\n",
    "n_valtest = len(dataset_unaug) - n_train\n",
    "train_set, valtest = random_split(dataset_unaug, [n_train, n_valtest])\n",
    "\n",
    "# Now split valtest again to get validation and test sets\n",
    "n_val = len(valtest) // 2\n",
    "n_test = len(valtest) - n_val\n",
    "validation_set, test_set = random_split(valtest, [n_val, n_test])\n",
    "\n",
    "\n",
    "# Only use data augmentation on the training set to avoid data leakage\n",
    "\n",
    "# Now append the augmented dataset to train_set\n",
    "train_combined_set = ConcatDataset([train_set, dataset_aug])\n",
    "print(len(train_combined_set))\n",
    "print(len(validation_set))\n",
    "print(len(test_set))\n",
    "\n",
    "# Create DataLoaders for each\n",
    "batch = 32\n",
    "train_loader = DataLoader(train_set, batch_size=batch, shuffle=True)\n",
    "val_loader = DataLoader(validation_set, batch_size=batch, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=batch, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0adb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(CNN, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 6, 5) # Conv2d(1, 6, 5) -> (input has 1 channel because grayscale, 6 filters as output, the filter_size is (5,5))\n",
    "            self.pool = nn.MaxPool2d(2, 2) \n",
    "            self.conv2 = nn.Conv2d(6, 64, 3)\n",
    "            self.conv3 = nn.Conv2d(64, 128, 2)\n",
    "            # note: we will pool again here, but since same pooling function used, don't redefine, just clarify this order in forward func (this is what will actually happen when model is trained)\n",
    "            self.fc1 = nn.Linear(128 * 2 * 2, 150) #fully connected layers\n",
    "            self.fc2 = nn.Linear(150, 84)\n",
    "            self.fc3 = nn.Linear(84, 8) # because 8 classes in kaggle dataset\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.pool(F.relu(self.conv1(x))) \n",
    "            x = self.pool(F.relu(self.conv2(x))) # can print shape of x, to debug and see what expected input/output sizes are\n",
    "            x = self.pool(F.relu(self.conv3(x)))\n",
    "            x = x.view(-1, 128*2*2)   # returns a \"view\" (reshape) of the tensor without copying (in this case we flatten)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "        \n",
    "model = CNN()\n",
    "\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=0.001)\n",
    "model.optimizer = opt\n",
    "model.loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063f666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194c25f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbb8b56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cai4104",
   "language": "python",
   "name": "cai4104"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
